# Decision Optimization for Glucose Control - Type 1 Diabetes

## Overview 
Explored RL4T1D framework developed by Hettiarachchi et al. to evaluate the success of the Proximal Policy Optimization (PPO) RL algorithm for automated insulin delivery in Type 1 Diabetes care.

## Methods
- Simulated patient glucose-insulin dynamics using the Simglucose virtual environment.
- Implemented PPO agents to learn insulin dosing policies, compared to a clinical baseline.
- Evaluated performance using Time in Range (TIR) and blood glucose risk metrics (LGBI & HGBI)

## Key Insights
- RL agent learning was unstable due to environment configuration issues (missed meals, early termination)
- Clinical baseline was safer but less adaptable
- Demonstrated ability to interpret complex simulation outputs, identify errors, and troubleshoot

## Outcome
Produced a comprehenisve final report with a complete literature review and summaries of methodology, experiments, and limitations.

## Tools
Python, PyTorch, Simglucose Simulator, Windows PowerShell
